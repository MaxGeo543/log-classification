{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64cadb6c",
   "metadata": {},
   "source": [
    "# Log Classification using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d638ca",
   "metadata": {},
   "source": [
    "## Importing relevant libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1458ddf2",
   "metadata": {},
   "source": [
    "In this first cell all necessary classes and functions get imported. Under the hood libraries like keras, sklearn and tensorflow are used but their uses are wrapped in custom methods and classes.\n",
    "\n",
    "The most important things that are imported in this step are the following: \n",
    "- `get_preprocessor` either loads a **Preprocessor** object from a file or creates a new one. The preprocessor contains all information about how data should be preprocessed and brought into a machine readable format. If a new preprocessor is created it will automatically save it to a file to reuse afterwards. This is useful since initializing the used encoders may take some time and loading them from a file should be significantly faster. \n",
    "- `get_dataset` either loads a **Dataset** object from a file or creates a new one. The dataset gets created using a preprocessor object which loads log files, preprocesses them and saves them to the dataset. The preprocessing is the most time consuming, therefore it is recommended to load datasets from a file if possible.\n",
    "- `get_model` creates a new **Classifier** object using a preprocessor and a dataset. The dataset will be split and used for training, validation, evaluation and optionally data augmentation. Once the classifier is trained it can be used to classify raw log data. \n",
    "- several encoders that can be passed to the preprocessor to encode data. All encoders are imported even though you will never use all of them at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_model import get_preprocessor, get_dataset, get_model\n",
    "from encoders.datetime_encoder import DatetimeEncoder\n",
    "from encoders.datetime_features import DatetimeFeature\n",
    "from encoders.loglevel_encoder import LogLevelOrdinalEncoder, LogLevelOneHotEncoder\n",
    "from encoders.message_encoder import MessageTextVectorizationEncoder, MessageBERTEmbeddingEncoder, MessageBERTEncoder\n",
    "from encoders.function_encoder import FunctionLabelEncoder, FunctionOneHotEncoder, FunctionOrdinalEncoder\n",
    "from encoders.classes_encoder import ClassesLabelBinarizer, ClassesLabelEncoder\n",
    "\n",
    "from util import get_sorted_log_numbers_by_size\n",
    "from data_augmentation import remove_and_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a582801",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## General configuration and initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c97d6a",
   "metadata": {},
   "source": [
    "The path to preprocessor and dataset files are initialized to `None` which means that new ones will be created, this can be optionally changed in a later cell to load them from files instead. \n",
    "\n",
    "`VERBOSE` means that more information will be printed to the console or in this case to this notebook. This is True by default, but can be changed to False here if it is unwanted.\n",
    "\n",
    "`EVALUATION_NAME` is the file path and name of files used for evaluation purposes. Several text and image files will be saved, there may be different suffixes added to the name to differentiate between them. \n",
    "\n",
    "`MODEL_TYPE` can be either lstm or transformer. As of now the lstm models achieve significantly better results.\n",
    "\n",
    "`LOGS_PER_CLASS` will be used when preprocessing the log files and when creating a dataset. It defines how many objects of each class should be attempted to be loaded. While using more data is good, some classes are underrepresented. High LOGS_PER_CLASS has a higher risk of carrying over the underrepresentation to the final dataset. \n",
    "\n",
    "`LOGS_TO_LOAD` is a list of numbers of log files to load. You can use the helper function `get_sorted_log_numbers_by_size` to get the smallest or largest log files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSOR_PATH = DATASET_PATH = None\n",
    "\n",
    "VERBOSE = True\n",
    "EVALUATION_NAME = \"evaluations/test\"\n",
    "MODEL_TYPE = \"lstm\" # or transformer\n",
    "LOGS_PER_CLASS = 200\n",
    "LOGS_TO_LOAD = get_sorted_log_numbers_by_size(\"./data/CCI\")[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4245f503",
   "metadata": {},
   "source": [
    "Here you may optionally set paths to preprocessor or dataset paths. Simply uncomment one of the lines or define the path yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a937132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# large dataset (200 logs per class, 50 logs per window)\n",
    "# PREPROCESSOR_PATH = \"./preprocessors/[Etxk7-u9DWzAAcFS][20250819_150015]train_model_pp.json\"\n",
    "# DATASET_PATH = \"./data/datasets/[20250821_110453]dataset.npz\"\n",
    "\n",
    "# smaller dataset (100 logs per class, 20 logs per window)\n",
    "# PREPROCESSOR_PATH = \"./preprocessors/[Etxk7-u9DWzAAcFS][20250829_112751]train_model_pp.json\"# r\"preprocessors\\[Etxk7-u9DWzAAcFS][20250821_130907]train_model_pp.json\"\n",
    "# DATASET_PATH = \"./data/datasets/[20250829_120602]dataset.npz\"#r\"data\\datasets\\[20250821_134533]dataset.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2af608",
   "metadata": {},
   "source": [
    "## Preprocessor, Model and Training Configuration\n",
    "Here the preprocessor, used models and training can be configured. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437430d3",
   "metadata": {},
   "source": [
    "### Preprocessor Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0660311d",
   "metadata": {},
   "source": [
    "- `name` is a string to identify preprocessors in a human readable way, the name will be present in the filename if the preprocessor is saved\n",
    "- `window_size` is how many log entries one data point contains. If the window size is 50 for example, each data point will have 50 log entries. If a single log entry has dimension `n`, the data point will have dimension `window_size x n`\n",
    "- `classes_encoder`, `message_encoder`, `log_level_encoder`, `function_encoder`, `datetime_encoder` are the encoders used by the preprocessor to encode different types of features to machine readable format (numerical tensors) The different types of encoders have different arguments you can pass to them. \n",
    "- The `DatetimeEncoder` is special since there is only one kind that is configured by the argument you pass to it. It takes a list of `DatetimeFeature`s that define which part of the feature to use and how to preprocess them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde91db",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  Long Short Term Memory Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816fd64a",
   "metadata": {},
   "source": [
    "The `LSTM_CONFIG` will be used only if the `MODEL_TYPE` set previously is `lstm`. It defines how the LSTM model that is used should be built. \n",
    "- The LSTM Model will be built with `layers` layers, each having `units` LSTM units\n",
    "- `dropout` is the probability that some weights in the LSTM layers will be set to 0 during training to prevent overfitting, `recurrent_drouput` is the same during the recurrent step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd17abb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Transformer Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d0460",
   "metadata": {},
   "source": [
    "The `TRANSFORMER_CONFIG` will be used only if the `MODEL_TYPE` set previously is `transformer`. It defines how the transformer model that is used should be built. \n",
    "- `transformer_layers` is the amount of layered transformer blocks to use\n",
    "- `model_dim` is the dimension of the transformer blocks\n",
    "- `num_heads` is the amount of attention heads used for self attention\n",
    "- `ff_dim` is the dimension of the FFN inside the transformer blocks\n",
    "- `dropout` is the probability that some weights will be set to 0 during training to prevent overfitting\n",
    "- `hidden_units` is the dimension of the output in the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c7a4a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Classifier Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ad9f3",
   "metadata": {},
   "source": [
    "The `CLASSIFIER_CONFIG` contains general configuration settings for the classifier mainly used for training. Since they are used for training you could argue that they are Training configurations, however the Classifier needs these values at initialization and not only when training.\n",
    "- `data_split_ratios` is a tuple defining a ratio by which to split the dataset. The wights of the split are in the following order\n",
    "  - **training data** to train the model on\n",
    "  - **validation data** to validate during training\n",
    "  - **test data** for evaluation after training\n",
    "  - **pseudolabel data** to be used without labels during Semi Supervised Learning with Pseudolabels\n",
    "- `learning_rate` is the learning rate used by the optimizer\n",
    "- `metrics` is a list of metrics to be tracked when training the model\n",
    "- `seed` is a seed for random modules you may set, optionally use `None` for no seed/random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c02c338",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a3f42d",
   "metadata": {},
   "source": [
    "- `epochs` is the maximum amount of epochs to train for\n",
    "- `batch_size`\n",
    "- parameters prefixed with `es_` are used for Early Stopping, so that when a criteria is filled, the training stops before the maximum amount of `epochs`. If you do not want to use Early Stopping, set `es_monitor` to `None`\n",
    "  - `es_monitor` is the Quantity to be monitored, if this is None no EarlyStopping will be applied\n",
    "  - `es_patience` defines how many epochs without change are needed to trigger EarlyStopping, If None is specified it will default to epochs//10\n",
    "  - `es_min_delta` is the minimum change required to be considered an improvement\n",
    "  - `es_restore_best` defines whether to restore the best weights of the model after an EarlyStopping was triggered. If None is specified it will default to True.\n",
    "- parameters prefixef with `pl_` are used for pseudolabeling\n",
    "  - `pl_unlabeled_data` is additional unlabeled data used for pseudolabeling. Either this has to be defined or a split for pseudolabeling when initializing, to enable Pseudolabeling. You may also define both.\n",
    "  - `pl_interval` is the epoch interval of trying to add new data from pseudolabeling to the training data. \n",
    "  - `pl_confidence_threshold` is how much confidence a prediction is required to have for it to be considered for a pseudolabel and added to the labeled data\n",
    "- `data_augmentation_functions` is a list of tuples of floats and callables. The floats define the probability that a pseudolabeling function will be used. The function needs to accept four parameters: x, y, all_x, all_y and return a tuple new_x, new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfae4c9",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c86bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only used if PREPROCESSOR_PATH is None and a new preprocessor is created\n",
    "PREPROCESSOR_CONFIG = {\n",
    "    \"name\": \"test_pp\",\n",
    "    \"window_size\":       50,\n",
    "    \"classes_encoder\":   ClassesLabelEncoder(),\n",
    "    \"message_encoder\":   MessageBERTEmbeddingEncoder(output_mode=\"cls\"),\n",
    "    \"log_level_encoder\": LogLevelOneHotEncoder(),\n",
    "    \"function_encoder\":  FunctionOrdinalEncoder(),\n",
    "    \"datetime_encoder\":  DatetimeEncoder([\n",
    "                            DatetimeFeature.hour.cyclic.normalized\n",
    "                         ])\n",
    "}\n",
    "\n",
    "# only used if MODEL_TYPE == \"lstm\"\n",
    "LSTM_CONFIG = {\n",
    "    \"layers\":              1,\n",
    "    \"units\":              50,\n",
    "    \"dropout\":           0.0,\n",
    "    \"recurrent_dropout\": 0.0,\n",
    "}\n",
    "\n",
    "# only used if MODEL_TYPE == \"lstm\"\n",
    "TRANSFORMER_CONFIG = {\n",
    "    \"transformer_layers\": 2,\n",
    "    \"model_dim\":          128,\n",
    "    \"num_heads\":          4,\n",
    "    \"ff_dim\":             256,\n",
    "    \"dropout\":            0.1,\n",
    "    \"hidden_units\":       64,\n",
    "}\n",
    "\n",
    "\n",
    "CLASSIFIER_CONFIG = {\n",
    "    \"data_split_ratios\": (6, 1, 1, 0),\n",
    "    \"learning_rate\":     0.001,\n",
    "    \"metrics\":           [\"accuracy\"],\n",
    "    \"seed\":              None,\n",
    "}\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    \"epochs\":                  1000, \n",
    "    \"batch_size\":              32,\n",
    "\n",
    "    \"es_monitor\":              \"val_loss\",\n",
    "    \"es_patience\":             500,\n",
    "    \"es_restore_best\":         True,\n",
    "    \"es_min_delta\":            0.001,\n",
    "\n",
    "    \"pl_unlabeled_data\":       None,\n",
    "    \"pl_interval\":             10,\n",
    "    \"pl_confidence_threshold\": 32,\n",
    "\n",
    "    \"data_augmentation_functions\": [\n",
    "        (0.2, lambda x, y, _1, _2: remove_and_pad(x, y))\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd47813",
   "metadata": {},
   "source": [
    "## Loading Preprocessor and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = get_preprocessor(\n",
    "    PREPROCESSOR_PATH or PREPROCESSOR_CONFIG,\n",
    "    LOGS_TO_LOAD,\n",
    "    verbose=VERBOSE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0766e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(\n",
    "    DATASET_PATH or preprocessor, \n",
    "    LOGS_TO_LOAD,\n",
    "    verbose=VERBOSE, \n",
    "    logs_per_class=LOGS_PER_CLASS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87faf80",
   "metadata": {},
   "source": [
    "## Loading and Training a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(MODEL_TYPE, \n",
    "                  LSTM_CONFIG if MODEL_TYPE == \"lstm\" else TRANSFORMER_CONFIG,\n",
    "                  CLASSIFIER_CONFIG,\n",
    "                  preprocessor,\n",
    "                  dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16125348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model.train(**TRAINING_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02489641",
   "metadata": {},
   "source": [
    "## Evaluating the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3d7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "model.evaluate(EVALUATION_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
